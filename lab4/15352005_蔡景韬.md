<center><font size=6em>**中山大学移动信息工程学院本科生实验报告**</font></center>

<center><font size=5em>**（2017年秋季学期）**</font></center>

**课程名称：人工智能**

------

| **年级** | **专业方向** |  **学号**  | **姓名** |
| :----: | :------: | :------: | :----: |
|  1501  | 移动（互联网）  | 15352005 |  蔡景韬   |

------

### 一、实验题目

- 决策树

### 二、实验内容

#### 1. 算法原理

- **决策树（Decision Tree）**

  - 决策树是一种常见的基于机器学习的**分类方法**，与KNN分类和NB分类同属于**有监督的机器学习模型**

  - **决策树和PLA算法等线性分类器的区别**

    - 决策树是一种树形模型，是一个一个特征进行处理的。它对每一个特征做一个划分，相较于线性分类器，决策树可以找到非线性分割，且树形模型更加接近人的思维方式，可以产生可视化的分类规则，产生的模型具有可解释性（可以抽取规则）
    - 而线性分类器（包括感知机，线性回归，逻辑回归）是所有特征给予权重相加得到一个新的值，并通过阈值进行划分决策。线性分类器只能找到线性分割（除非对x进行多维映射）

  - 决策树代表的是对象属性与对象值之间的一种映射关系

    - 树中每个节点表示某个对象属性
    - 而每个分叉路径则代表某个可能的属性值
    - 而每个叶节点则对应从根节点到该叶节点所经历的路径（对象属性的情况）所表示的对象的值。

  - 决策树通常使用CHAID、CART、Quest和C5.0等算法进行建树：

    - **CART (Classification And Regression Trees) **是结合分类树分析和回归树分析的一种算法，它基于GINI系数进行决策选择属性
    - **C5.0**算法是**C4.5**算法的商业化版本，因此算法细节因版权问题尚未公开。
      - **ID3 (Iterative Dichotomiser 3) ** 是针对离散属性取值的一种算法，它根据信息增益进行决策选择属性
      - **C4.5**是对**ID3**的一种优化算法，它根据信息增益率进行决策选择属性，且适用于对连续数据的处理（能够通过信息增益率选择最佳的离散划分）
    - 本次实验使用**ID3**、**C4.5**、**CART**进行建树

  - **决策树的建树**

    - 决策树的建树思想，其实就是**寻找最纯净的划分方法**，即**纯度**最大，使目标变量要分得足够开，如ID3使用信息增益作为不纯度、C4.5使用信息增益率作为不纯度。
    - 决策树建树过程中，每次都选取最优的属性作为划分，采取的是一种自顶向下的贪心策略
    - 决策树每个叶节点即为一连串法则的分类结果

    0. 以资料母群体为根节点
    1. 遍历每个决策条件，对当前结果集进行划分
    2. 做单因子变异数分析，找出变异量最大的变项作为分割（使用上述的算法进行分析）
    3. 递归执行1、2两步，直至达到边界条件，标记为叶子节点
       1. 结果集中的所有数据属于同一个label，预测结果为这个label
       2. 当前特征集为空集，预测结果为结果集中的众数label
       3. 结果集在所有特征集中特征的取值相同，预测结果为结果集中的众数label

  - **决策树的优化**

    - 目的：为了提升决策树的**泛化性能**（对新鲜样本的适应能力）与解决对训练集**过拟合**问题
    - 决策树的重要参数都是防止过拟合的，主要有两个参数：**叶子节点的样本数**和**深度**
      - **叶子节点样本数：**经验值必须大于100，如果一个节点没有100个样本支持它的决策，一般被认为是过拟合的
      - **深度：**控制决策树的规模，深度太深一方面会导致过拟合，另一方面对于我们的理解是有难度的
    - **剪枝**
      - 原因：为了尽可能正确的分类训练样本，节点划分过程将不断重复，有时会造成决策树分支太多，这时就可能因训练样本吻合的太过，以至于把训练集自身的一些特点当作所有数据的特点而导致过拟合。
      - 剪枝是决策树学习算法减少“过拟合”情况的主要手段。
      - **预剪枝**
        - 过程：在树的生长过程中设定一个指标（如决策树深度、纯度等），当达到该指标时就停止生长。
        - 缺点：容易产生“视界局限”，一旦停止分支，断绝了其后继节点进行“好”分支操作的任何可能性。
      - **后剪枝**
        - 过程：首先让决策树充分生长，因而能克服“视界局限”，然后对所有相邻的成对叶节点考虑是否消去它们，如果消去能引起令人满意的结果（如错误率、不纯度、模型复杂度等），那么执行消去，令它们的公共父节点成为新叶节点
        - 优点：克服了“视界局限”效应
        - 缺点：计算量代价大，在大样本集中不适用
    - **随机森林**
      - 原因：尽管有剪枝的方法，但一棵树的生成肯定不如多棵树，因此就有了随机森林
      - 随机森林可以解决决策树泛化能力弱的缺点
      - 过程：
        1. **样本的随机：**从训练集中随机选取n个样本（未划分验证集的训练集）
        2. **特征的随机：**从所有属性中随机选取K个属性，建立决策树（K的经验值为$\sqrt{A} $ ；A是属性的个数）
        3. 重复以上两步m次，产生**m棵决策树**
        4. 这m棵决策树形成随机森林，每个测试集的样本通过m次判别，并进行投票表决
      - 随机森林的调参有：棵树m、最大深度（经验值不超过8层）等

- **信息熵和信息增益**

  - **信息熵**
    - 信息熵是信息论中的基本概念
    - 在实际通信前，信宿对于信源会发出什么信息不可能知道，称为信宿对信源状态具有不确定性，由于这种不确定性是发生在通信之前的，故称为先验不确定性。在收到信息后的不确定性，称为后验不确定性。如果先验不确定性等于后验不确定性，则表示信息量为零；如果后验不确定性等于零，则表示信宿收到了信源的全部信息。
    - 所以**信息是指对不确定性的消除**
    - **信息量**由消除的不确定性来确定
      - 表达式定义为：$-log_2P()$
      - 信息量单位是bit
    - **信息熵**是信息量的数学期望
      - 于1948年由香农引入，将其定义为随机事件出现的概率
      - 一个系统越是有序，信息熵就越低，反之一个系统越是混乱，它的信息熵就越高。所以信息熵可以被认为是**系统有序化程度的一个度量**
  - **信息增益**
    - 信息熵又称为先验熵，是在信息发送前信息量的数学期望
    - 后验熵指在信息发送后，信宿角度对信息量的数学期望
    - 一般先验熵大于后验熵，先验熵与后验熵估差，即所谓的信息增益
    - **信息增益**，反映的是信息消除随机不确定性的程度

- **ID3算法**

  - ID3算法的核心思想是以信息增益来度量属性的选择，选择分裂后信息增益最大的属性进行分裂

  - ID3算法需要计算的两个熵值

    - **原始信息熵：所有样本中各种label出现的不确定性之和。**根据熵的概念，熵越大，不确定性就越大，把事情搞清楚所需要的信息量就越多。
    - **属性的信息熵：每个属性的信息熵相当于一种条件熵，表示的是在某种属性的条件下，各种label出现的不确定性之和。**属性的信息熵越大，表示这个属性中拥有的样本类别越不“纯”。

  - 算法过程：

    1. 计算数据集D的原始信息熵（本次实验中只有两种label，所以求和公式只有两项）

       $H(D) = -\sum_{d\in D} p(d)log_2p(d)$

    2. 遍历所有属性，选出信息增益最大的属性作为决策点（划分）

       1. 计算属性X对数据集D的条件熵（x是属性X的所有取值）（本次实验中只有两种label，所以D的求和公式只有两项）

          $\begin{align*}H(D|X) &= \sum_{x\in X} p(x)H(D|X=x)\\&=\sum_{x\in X} p(x) \bigg [-\sum_{d\in D}p(d|x)log_2p(d|x) \bigg] \end{align*} $

       2. 计算信息增益

          $Gain(D,X) = H(D)-H(D|X)$

  - 缺点：

    - 只考虑属性变量是离散型
    - 会偏向选择取值多的属性
      - 如，当属性存在如id类的属性时，计算出条件熵为0，根据程序发现是最优决策点，然而实际上毫无意义

- **C4.5算法**

  - C4.5算法是基于ID3算法进行改进的一种算法

    - 使用信息增益率选择属性，克服了用信息增益选择属性时偏向选择取值多的属性的不足
    - 能够适应非离散数据（能够通过信息增益率选择最佳的离散划分）（本次实验没有用到）

  - **分裂信息度量（SplitInfo）**

    - 用分裂信息来考虑某种属性进行分裂时分支的数量信息和尺寸信息，我们把这些信息称为属性的内在信息
    - $信息增益率 = \frac{属性X的信息增益}{属性X的分裂信息}$
    - 信息增益率（属性的重要性）会随着内在信息的增大而减小（也就是说，如果这个属性本身不确定性就很大，那就越不倾向选择它）
    - 对于上文ID3的缺点，当出现id类的属性，由于这个属性本身不确定很大（每个样本有一个id），会导致内在信息很大，从而使信息增益率减小，程序就不倾向选择
    - 由此可以看出，引入分裂信息得到的信息增益率是对单纯使用信息增益有所补偿

  - 算法过程：

    1. 计算数据集D的原始信息熵（本次实验中只有两种label，所以求和公式只有两项）

       $H(D) = -\sum_{d\in D} p(d)log_2p(d)$

    2. 遍历所有属性，选出信息增益率最大的属性作为决策点（划分）

       1. 计算属性X对数据集D的条件熵（x是属性X的所有取值）（本次实验中只有两种label，所以D的求和公式只有两项）

          $\begin{align*}H(D|X) &= \sum_{x\in X} p(x)H(D|X=x)\\&=\sum_{x\in X} p(x) \bigg [-\sum_{d\in D}p(d|x)log_2p(d|x) \bigg] \end{align*} $

       2. 计算信息增益

          $Gain(D,X) = H(D)-H(D|X)$

       3. 计算分裂信息

          $SplitInfo(D,X)=-\sum_{j=1}^{v}\bigg(\frac{|D_j|}{|D|}\ \times log_2\frac{|D_j|}{|D|}\bigg)$

       4. 计算信息增益率

          $GainRatio(D,X)=\frac{Gain(D,X)}{SplitInfo(D,X)}$

  - 缺点：

    - 在构造树的过程中，需要多次对数据集进行多次的顺序扫描和排序，导致算法的低效（ID3亦是同样的缺点）

- **CART (Classification And Regression Tree) 算法**

  - CART算法使用GINI指数进行决策选择属性

  - **GINI指数**

    - 定义：在样本集合中一个随机选中的样本被分错的概率，即

      基尼指数（基尼不纯度）= 样本被选中的概率 \* 样本被分错的概率

    - 公式：$Gini(p)=\sum_{k=1}^Kp_k(1-p_k)=1-\sum_{k=1}^Kp_k^2$

    - CART算法使用GINI指数作为划分指标的原因：（引用自知乎）

      >ID3算法使用了信息增益来选择特征，信息增益大的优先选择。
      >
      >C4.5算法中，采用了信息增益率来选择特征，解决ID3容易选择特征值多的特征。
      >
      >但是无论是ID3还是C4.5，都是基于信息论的熵模型的，这里面会涉及大量的对数运算。能不能简化模型同时也不至于完全丢失熵模型的优点呢？有！CART分类树算法使用基尼系数来代替信息增益比
      >
      ><img src="picture\2.jpg">
      >
      >​
      >
      >从上图可以看出，基尼系数和熵之半的曲线非常接近，仅仅在45度角附近误差稍大。因此，基尼系数可以做为熵模型的一个近似替代。

  - **二路分裂的CART**

    - 原先的CART算法是一种**二分递归分割**技术：把当前样本划分为两个子样本，使得生成的每个非叶子结点都有两个分支

    - 因此CART算法生成的决策树是结构简洁的二叉树

    - 算法过程：

      遍历所有属性，选取GINI系数**最小**的属性作为决策点（划分）

      1. 把属性X的取值全排划分为两部分，总共有$C_n^1+C_n^2+...+C_n^{\frac{n}{2}}$种情况

      2. 对每种情况计算GINI系数

         $Gini(D,X)=\sum_{j=1}^v\bigg(p(X_j)\times gini(D_j|A=A_j)\bigg)$

         其中，$gini(D_j|A=A_j)=1-\sum_{i=1}^2p_i^2$

      3. 选取最优的划分情况（该情况算出的Gini值最小）作为属性X的GINI系数

  - **多路分裂的CART（本次实验所用的CART算法）**

    - 算法过程：

      - 遍历所有属性，选取GINI系数**最小**的属性作为决策点（划分）

        - 计算属性X的GINI系数

          $Gini(D,X)=\sum_{j=1}^v\bigg(p(X_j)\times gini(D_j|A=A_j)\bigg)$

          其中，$gini(D_j|A=A_j)=1-\sum_{i=1}^np_i^2$

#### 2. 伪代码

- **处理连续型特征**

  ```c++
  for i : 属性
  	if 属性i的取值数目 >= 10 种
  		for j : 数据集
  			每个样本属性i的值都 除以 log2(取值数目)
          for j : 测试集
          	每个样本属性i的值都 除以 log2(取值数目)
  ```

- **划分训练集与验证集**

  ```c++
  对数据集随机洗牌 random_shuffle()
  训练集 = 数据集的前75%
  验证集 = 数据集的后25%
  ```

- **ID3算法**

  ```c++
  for i : 属性集Attr
  	计算当前数据集的信息熵entropy_original：
  		for i : 数据集Data
  			计数positive: label为1的数目
  			计数negative: label为-1的数目
  		return 信息熵 = -正label的概率*log2(正的概率)-负label的概率*log2(负的概率)
      计算属性i的条件熵entropy:
  		for j : 属性i的取值（不重复）
  			for k : 数据集Data
  				计数positive: 属性取值为j的样本中label为1的数目
  				计数negative: 属性取值为j的样本中label为-1的数目
  			return 各取值的熵 = -正label的概率*log2(正的概率)-负label的概率*log2(负的概率)
          return 条件熵 = 求和｛ 各取值概率 * 各取值的熵 ｝
  	计算属性i的信息增益Gain:
  		return Gain = entropy_original - entropy
  return 最大的信息增益Gain_MAX对应的属性i
  ```

- **C4.5算法**

  ```c++
  for i : 属性集Attr
  	计算属性i的信息增益Gain:
  		return Gain = entropy_original - entropy
  	计算属性i的分裂信息SplitInfo:
  		return 分裂信息 = 求和｛ -属性i各取值的概率*log2(属性i各取值的概率) ｝
  	计算属性i的信息增益率GainRadio:
  		return GainRadio = Gain / SplitInfo 
  return 最大的信息增益率GainRadio_MAX对应的属性i
  ```

- **CART算法**

  ```c++
  for i : 属性集Attr
  	计算属性i的GINI系数
  		for j : 属性i的取值（不重复）
          	for k : 数据集Data
          		计数positive: 属性取值为j的样本中label为1的数目
  				计数negative: 属性取值为j的样本中label为-1的数目
  			return 各取值正label的概率和负label的概率
  		return GINI系数 = 求和｛ 各取值概率 * (1-各取值正的概率-各取值负的概率) ｝
  return 最小的GINI系数GINI_MIN对应的属性i
  ```

- **边界条件**

  ```c++
  for i : 当前数据集Data
  	计数positive：label为1的个数
  	计数negative: label为-1的个数
  if 都是正的label或者都是负的label
  	都是正的label return 1
    	都是负的label return -1
  if 属性集Attr 为 空
  	返回较多的lablel取值:
  		if positive > negative  return 1 ;
  		else return -1 ;
  for i : 属性集Attr
  	for j : 当前数据集Data
  		if 每个样本的对于每一个属性i的所有取值相同
  			return 较多的lablel取值
  ```

- **根据属性分割数据集**

  ```c++
  对某一列属性sort
  for i : sort后的数据集
  	在属性取值的分界将数据集分隔开来
  ```

- **递归建树**

  ```c++
  if 满足边界条件
  	将节点p标记为叶子节点
  	节点p的预测结果为函数返回的结果
  	节点p的众数lable为函数返回的结果

  attr_choosen : 选择最优属性

  subsets : 根据属性分割数据集

  从属性集Attr中删除attr_choosen

  设置节点p的信息：
  	设置节点p的众数label
  	节点p的选择属性为attr_choosen
  	节点p的子节点个数为subsets的大小
  if 节点p的深度 > 8 
  	return 

  for i : subsets
  	创建子节点subNode: 
  		设置子节点的属性取值: 数据集子集i的每个样本在attr_choosen这一列的取值
  		子节点的深度 = 节点p的深度 + 1
     		将subNode插入节点p的子节点数组中
      swap(数据集,数据集子集i) ;
  	递归建树: 传入子节点subNode
  	swap(数据集,数据集子集i) ;

  将attr_choosen重新加到属性集Attr中

  return
  ```

- **根据决策树预测**

  ```c++
  for i : 测试集数据Data
  	创建Node p : 初始化为root
  	while p不是叶子节点
  		p选取的属性attr
  		if p的子节点数组中 没有 样本i在attr那一列的取值
  			预测结果为p的众数label
  			break
  		if p的子节点数组pi的属性取值 等于 样本i在attr那一列的取值
  			p = pi
  	if p是叶子节点
  		预测结果为节点p的预测结果
  ```

- **随机森林算法**

  - **多次建树**

    ```c++
    拷贝训练集train_copy = 全局变量训练集Train
    for i : 树的棵树

    	全局变量训练集Train = 空
    	for j : 拷贝训练集train_copy的大小
        	随机有放回的抽取数据: Train.push( train_copy[random()] )
              
        全局变量属性集Attribute = 空
        for j : 经验取值 sqrt(属性个数)
          随机不放回的抽取一个属性
          
        根据训练集与属性集递归建树
    ```

  - **根据森林预测**

    ```c++
    for i : 测试集数据Data
    	计数正结果个数positive
    	计数负结果个数negative
    	for j : 决策树棵树
    		创建Node p : 初始化为root[j]
    		while p不是叶子节点
    			p选取的属性attr
    			if p的子节点数组中 没有 样本i在attr那一列的取值
    				if p的众数label为正的 则positive计数++
                    else negative计数++
    				break
    			if p的子节点数组pi的属性取值 等于 样本i在attr那一列的取值
    				p = pi
    		if p是叶子节点
    			if 节点p的预测结果为正的 则positive计数++
                else negative计数++
    	预测结果为个数多的label（比较positive与negative）
    ```

#### 3. 关键代码截图

- **划分训练集与验证集**

  <img src="picture\2.png">

- **树结点**

  <img src="picture\1.png">

- **ID3算法**

  <img src="picture\3.png">

  <img src="picture\4.png">

  <img src="picture\5.png">

- **C4.5算法**

  <img src="picture\6.png">

  <img src="picture\7.png">

- **CART算法**

  <img src="picture\8.png">

  <img src="picture\9.png">

- **边界条件**

  <img src="picture\10.png">

- **根据属性分割数据集**

  <img src="picture\11.png">

  <img src="picture\12.png">

- **递归建树**

  <img src="picture\13.png">

  <img src="picture\14.png">

- **根据决策树预测**

  <img src="picture\15.png">

- **随机森林算法**

  <img src="picture\20.png">

  <img src="picture\21.png">

- 

#### 4. 创新点&优化

- 使用random_shuffle()对数据集随机洗牌，再抽取3/4作为训练集，1/4作为验证集
- 处理过多属性取值的属性时，当属性取值个数超过10种时，该属性所有值都除以$log_2(取值个数)$
- 使用随机森林

### 三、实验结果及分析

#### 1. 实验结果展示示例（使用小数据集）

- 训练集：

  |   age   | income | student | credit_rating | buy_computer |
  | :-----: | :----: | :-----: | :-----------: | :----------: |
  |  <=30   |  high  |   no    |     fair      |      no      |
  |  <=30   |  high  |   no    |   excellent   |      no      |
  | 31...40 |  high  |   no    |     fair      |     yes      |
  |   >40   | medium |   no    |     fair      |     yes      |
  |   >40   |  low   |   yes   |     fair      |     yes      |
  |   >40   |  low   |   yes   |   excellent   |      no      |
  | 31...40 |  low   |   yes   |   excellent   |     yes      |
  |  <=30   | medium |   no    |     fair      |      no      |
  |  <=30   |  low   |   yes   |     fair      |     yes      |
  |   >40   | medium |   yes   |     fair      |     yes      |
  |  <=30   | medium |   yes   |   excellent   |     yes      |
  | 31...40 | medium |   no    |   excellent   |     yes      |
  | 31...40 |  high  |   yes   |     fair      |     yes      |
  |   >40   | medium |   no    |   excellent   |      no      |

- 转化为输入格式的训练集

  <img src="picture\16.png">

- 验证集，对所有属性进行全排，并添加几个不存在的属性值，label列写出正确答案

- 可以划分训练集，并画出决策树

  - age + student

  | age  | income | student | credit_rating | buy_computer |
  | :--: | :----: | :-----: | :-----------: | :----------: |
  | <=30 |  high  |   no    |     fair      |      no      |
  | <=30 |  high  |   no    |   excellent   |      no      |
  | <=30 | medium |   no    |     fair      |      no      |
  | age  | income | student | credit_rating | buy_computer |
  | :--: | :----: | :-----: | :-----------: | :----------: |
  | <=30 |  low   |   yes   |     fair      |     yes      |
  | <=30 | medium |   yes   |   excellent   |     yes      |
  - age 

  | age     | income | student | credit_rating | buy_computer |
  | :------ | :----: | :-----: | :-----------: | :----------: |
  | 31...40 |  high  |   no    |     fair      |     yes      |
  | 31...40 |  low   |   yes   |   excellent   |     yes      |
  | 31...40 | medium |   no    |   excellent   |     yes      |
  | 31...40 |  high  |   yes   |     fair      |     yes      |
  - age + credit_rating

  | age  | income | student | credit_rating | buy_computer |
  | :--: | :----: | :-----: | :-----------: | :----------: |
  | >40  | medium |   no    |     fair      |     yes      |
  | >40  |  low   |   yes   |     fair      |     yes      |
  | >40  | medium |   yes   |     fair      |     yes      |

  | age  | income | student | credit_rating | buy_computer |
  | :--: | :----: | :-----: | :-----------: | :----------: |
  | >40  |  low   |   yes   |   excellent   |      no      |
  | >40  | medium |   no    |   excellent   |      no      |

  - 决策树

    <img src="picture\17.png">

- 执行程序，打印出决策树与验证集的准确率

  - 非叶子节点输出属性列数，叶子节点输出属性值*label

    <img src="picture\18.png">

  - 可以看到与画出的决策树相同，且对全排后的验证集准确率为1

  - 打印树代码

  ```c++
  void printTree(Node* root){
  	Node* p ;
  	int curDepth=0 ;
  	
  	queue<Node*> que,coutQue ;
  	que.push(root) ;
  	
  	while ( !que.empty() ){
  		p = que.front() ; que.pop() ;
  		
  		if ( p->children!=NULL )
  			for ( int i=0 ; i<p->children_number ; i++ )
  				que.push(p->children[i]) ;
  		
  		coutQue.push(p) ;
  	}
  	while ( !coutQue.empty() ){
  		p = coutQue.front() ; coutQue.pop() ;
  		if ( curDepth!=p->depth ) cout << "\n" , curDepth++ ;
  		if ( p->isLeaf ) cout << p->AttributeValue << "*" << p->ans ;
  		else cout << p->Attribute ;
  		cout << " " ;
  	}
  }
  ```


####2. 评测指标展示即分析（如果实验题目有特殊要求，否则使用准确率）

- 使用限制最深深度为8层，随机划分3/4、1/4分别为训练集与验证集

  - 分别对ID3、C4.5与CART算法执行100次独立实验，结果如下

    <img src="picture\19.png">

  - 平均准确率为

    |   ID3   |   C4.5   |   CART   |
    | :-----: | :------: | :------: |
    | 0.57401 | 0.564518 | 0.581218 |

- 使用限制最深深度为8层，100棵决策树，每棵决策树的属性个数为3，随机划分3/4、1/4分别为训练集与验证集的随机森林算法

  - 分别对ID3、C4.5与CART算法执行100次独立实验，结果如下

    <img src="picture\22.png">

  - 平均准确率为

    |   ID3    |   C4.5   |   CART   |
    | :------: | :------: | :------: |
    | 0.586751 | 0.598985 | 0.600254 |

  - 比较

    - 可以看到，随机森林对于平均准确率三个模型都有所提升，且在100次测试中，准确率更是有升至0.7


### 四、思考题

#### 1. 决策树有哪些避免过拟合的方法

- 调节决策树的参数，防止过拟合
  - 主要有两个参数：**叶子节点的样本数**和**深度**
  - **叶子节点样本数：**经验值必须大于100，如果一个节点没有100个样本支持它的决策，一般被认为是过拟合的
  - **深度：**控制决策树的规模，深度太深一方面会导致过拟合，另一方面对于我们的理解是有难度的
- 进行剪枝也可以防止过拟合
  - **剪枝**是决策树学习算法减少“过拟合”情况的主要手段。
  - **预剪枝**
  - **后剪枝**
- 使用随机森林也能防止过拟合
  - **随机森林**不仅能防止过拟合，还可以解决决策树泛化能力弱的缺点

####2. C4.5相比于ID3的优点是什么？

- C4.5算法是基于ID3算法进行改进的一种算法
  - 使用信息增益率选择属性，克服了用信息增益选择属性时偏向选择取值多的属性的不足
  - 能够在在树构造过程中进行悲观剪枝（本次实验中没有用到）
  - 能够适应非离散数据（能够通过信息增益率选择最佳的离散划分）（本次实验没有用到）
  - 能够对不完整数据进行处理（对属性缺失值，取父节点的众数label）

#### 3. 如何用决策树来判断特征的重要性？

- 决策树中越靠近根节点的属性节点越重要




